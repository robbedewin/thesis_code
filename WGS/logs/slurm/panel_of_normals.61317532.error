Assuming unrestricted shared filesystem usage.
Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 16
Rules claiming more threads will be scaled down.
Conda environments: ignored
Job stats:
job            count
-----------  -------
mutect_pon2        1
mutect_pon3        1
total              2

Select jobs to execute...
Execute 1 jobs...

[Sun Apr  7 18:59:47 2024]
localrule mutect_pon2:
    input: resources/genome.fa, results/mutect2/pon/sample_map, resources/whole_genome.intervals
    output: results/mutect2/pon/pon_db
    log: logs/mutect2/pon/genomicsdbImport.log
    jobid: 3
    reason: Missing output files: results/mutect2/pon/pon_db
    resources: tmpdir=/tmp


        gatk GenomicsDBImport             -R resources/genome.fa             -L resources/whole_genome.intervals             --genomicsdb-workspace-path results/mutect2/pon/pon_db             --sample-name-map results/mutect2/pon/sample_map             2>logs/mutect2/pon/genomicsdbImport.log
        
ImproperOutputException in rule mutect_pon2 in file /lustre1/project/stg_00096/home/rdewin/WGS/rules/mutect2.smk, line 36:
Outputs of incorrect type (directories when expecting files or vice versa). Output directories must be flagged with directory(). for rule mutect_pon2:
    output: results/mutect2/pon/pon_db
    affected files:
        results/mutect2/pon/pon_db
Removing output files of failed job mutect_pon2 since they might be corrupted:
results/mutect2/pon/pon_db
Skipped removing non-empty directory results/mutect2/pon/pon_db
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2024-04-07T185946.146604.snakemake.log
WorkflowError:
At least one job did not complete successfully.
